{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\";\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import tensorflow as tf  # for data preprocessing\n",
    "from tensorflow import keras\n",
    "import nibabel as nib\n",
    "\n",
    "from scipy import ndimage\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Input,Dense,GlobalAveragePooling2D,Flatten,concatenate,BatchNormalization, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras import regularizers\n",
    "from collections import Counter\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading and ploting single nii file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"/home/Data/Datasets/Parkinson/radiological/PPMI/spect-mri/filtered/test/control/spect/\"\n",
    "file_path = root_path + \"3104/preprocessed/PPMI_3104_NM_Reconstructed_DaTSCAN_Br_20121011134355542_1_S117556_spect_resampled_registered.nii.gz\"\n",
    "my_nifti = nib.load(file_path).get_fdata()\n",
    "\n",
    "# get the shape of your NIfTI\n",
    "print(my_nifti.shape)\n",
    "\n",
    "# Rotate the image data 90 degrees counterclockwise\n",
    "rotated_img_data = np.rot90(my_nifti)\n",
    "\n",
    "# Choose the slice index you want to save\n",
    "slice_index = 78  # Change this to the index of the slice you want to save\n",
    "\n",
    "# Get the selected slice\n",
    "selected_slice = rotated_img_data[:, :, slice_index]\n",
    "\n",
    "# Repeat the slice along a new axis to create an RGB-like dimension\n",
    "rgb_slice = np.repeat(selected_slice[:, :, np.newaxis], 3, axis=2)\n",
    "\n",
    "print(f\"min value: {rgb_slice.min()}, max value: {rgb_slice.max()}\")\n",
    "print(f\"dimensions: {rgb_slice.shape}\")\n",
    "\n",
    "# # access it as 3D numpy array\n",
    "# nifti_slice = my_nifti[:,:,30]\n",
    "\n",
    "# # Rotate the slice 90 degrees counterclockwise\n",
    "# rotated_img_data = np.rot90(nifti_slice)\n",
    "\n",
    "# display the slice\n",
    "plt.imshow(selected_slice, cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the selected slice as a NumPy file\n",
    "np.save('rgb_slice.npy', rgb_slice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading and plotting the np slice saved\n",
    "np_file = \"/home/Data/franklin/Doctorado/parkinson/projects/parcellation_translation/notebooks/rgb_slice.npy\"\n",
    "np_image = np.load(np_file)\n",
    "print(f\"min value: {np_image.min()}, max value: {np_image.max()}\")\n",
    "print(np_image.shape)\n",
    "# Normalize the RGB data to the range [0, 1]\n",
    "rgb_slice_normalized = np_image / np.max(np_image)\n",
    "# Plot the array data\n",
    "plt.imshow(rgb_slice_normalized, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving npy files in a similar structure like the PNG ones. \n",
    "Filterin the slices regarding the following criteria:\n",
    "* Parkinson and control populations:\n",
    "\n",
    "\t* SPECT: * CONTROL---->[34, 47]\n",
    "\t\t * PD---->[29, 49]\n",
    "\n",
    "\t* MRI:\t * CONTROL---->[55, 68]\n",
    "    \t * PD---->[55, 68]\n",
    "\n",
    "* SWEDD populations\n",
    "\t* SPECT: [32, 50]\n",
    "\t* MRI: [51, 65]\n",
    "\n",
    "* PRODROMAL population------> **it needs to be revised**\n",
    "\t* MRI: []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_nii(file_path, low_idx, top_idx, save_path, case_name, modal_name):\n",
    "    \n",
    "    # get the nii data\n",
    "    my_nifti = nib.load(file_path).get_fdata()\n",
    "    # Rotate the nii data 90 degrees counterclockwise\n",
    "    rotated_img_data = np.rot90(my_nifti)  \n",
    "    # Choose the slice index you want to save\n",
    "    for slice_index in range(low_idx, top_idx+1, 1):\n",
    "        \n",
    "        # Get the selected slice\n",
    "        selected_slice = rotated_img_data[:, :, slice_index]\n",
    "\n",
    "        # Repeat the slice along a new axis to create an RGB-like dimension\n",
    "        rgb_slice = np.repeat(selected_slice[:, :, np.newaxis], 3, axis=2)\n",
    "        \n",
    "        # Save the selected slice as a NumPy file\n",
    "        directory = save_path + case_name + \"/\"\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "            \n",
    "        np_file_name = directory + case_name + \"_\" + modal_name + \"_slice_\" + str(slice_index) + \".npy\"\n",
    "        #print(f\"saving {file_path} on: {np_file_name}\")\n",
    "        np.save(np_file_name, rgb_slice)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For Parkinson and Control populations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"/home/Data/Datasets/Parkinson/radiological/PPMI/spect-mri/filtered\"\n",
    "split = \"test\"\n",
    "group = \"parkinson\"\n",
    "modality = \"spect\"\n",
    "source_path = root_path + \"/\" + split + \"/\" + group + \"/\" + modality\n",
    "cases = sorted(os.listdir(source_path))\n",
    "\n",
    "for case in cases:\n",
    "    print(case)\n",
    "    preprocessed_path = source_path + \"/\" + case + \"/preprocessed/\"\n",
    "    files = sorted(os.listdir(preprocessed_path))\n",
    "    \n",
    "    # preprocessed file identification and slices selection\n",
    "    if modality == \"spect\":\n",
    "        nii_file = [file for file in files if file.endswith(\"_resampled.nii.gz\")][0]\n",
    "        case_name = nii_file.split(\"_\")[1]\n",
    "        modal_name = nii_file.split(\"_\")[4]\n",
    "        nii_path = preprocessed_path + nii_file\n",
    "        \n",
    "        if group == \"control\":\n",
    "            low_slice = 34\n",
    "            top_slice = 47\n",
    "        else:\n",
    "            low_slice = 29\n",
    "            top_slice = 49\n",
    "    else:\n",
    "        nii_file = [file for file in files if file.endswith(\"rigid_registered.nii.gz\")][0]\n",
    "        case_name = nii_file.split(\"_\")[1]\n",
    "        modal_name = nii_file.split(\"_\")[3]\n",
    "        nii_path = preprocessed_path + nii_file\n",
    "        low_slice = 55\n",
    "        top_slice = 68\n",
    "        \n",
    "    # data preprocessing: nii rotatation, slice selection and numpy save\n",
    "    save_path = root_path + \"/\" + split + \"/\" + group + \"/\" + \"extension\" + \"/npyFiles/\" + modality + \"/\"\n",
    "    preprocess_nii(nii_path, low_slice, top_slice, save_path, case_name, modal_name)  \n",
    "    \n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For SWEDD populations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"/home/Data/Datasets/Parkinson/radiological/PPMI/spect-mri/filtered\"\n",
    "group = \"swedd\"\n",
    "modality = \"spect\"\n",
    "source_path = root_path + \"/\" + group + \"/\" + modality#---->\n",
    "cases = sorted(os.listdir(source_path))\n",
    "\n",
    "for case in cases:\n",
    "    print(case)\n",
    "    preprocessed_path = source_path + \"/\" + case + \"/preprocessed/\"\n",
    "    files = sorted(os.listdir(preprocessed_path))\n",
    "    \n",
    "    # preprocessed file identification and slices selection\n",
    "    if modality == \"spect\":\n",
    "        nii_file = [file for file in files if file.endswith(\"_registered.nii.gz\")][0]\n",
    "        case_name = nii_file.split(\"_\")[0]\n",
    "        modal_name = nii_file.split(\"_\")[5]\n",
    "        nii_path = preprocessed_path + nii_file\n",
    "        low_slice = 32\n",
    "        top_slice = 50\n",
    "               \n",
    "    else:\n",
    "        nii_file = [file for file in files if file.endswith(\"rigid_registered.nii.gz\")][0]\n",
    "        case_name = nii_file.split(\"_\")[1]\n",
    "        modal_name = nii_file.split(\"_\")[3]\n",
    "        nii_path = preprocessed_path + nii_file\n",
    "        low_slice = 59\n",
    "        top_slice = 108\n",
    "       \n",
    "     \n",
    "    # data preprocessing: nii rotatation, slice selection and numpy save\n",
    "    save_path = root_path + \"/\" + group + \"/\" + \"extension\" + \"/npyFiles/\" + modality + \"/\"\n",
    "    preprocess_nii(nii_path, low_slice, top_slice, save_path, case_name, modal_name) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_np_file(filepath):\n",
    "    \"\"\"Read and load volume\"\"\"\n",
    "    # Read file\n",
    "    scan = np.load(filepath)\n",
    "    #new lines\n",
    "    if scan.shape != (218, 182, 3):\n",
    "        scan = np.resize(scan, (218, 182, 3))\n",
    "    else:\n",
    "        None\n",
    "    \n",
    "    return scan\n",
    "\n",
    "def normalize(slice, path):\n",
    "    # Scale the data between 0 and 1\n",
    "    min_val = np.min(slice)\n",
    "    max_val = np.max(slice)\n",
    "    denominador = (max_val - min_val)\n",
    "    scaled_data = (slice - min_val) / denominador\n",
    "    \n",
    "    if denominador <=0:\n",
    "        print(\"ERROR\")\n",
    "        print(\"min_val: \", min_val)\n",
    "        print(\"max_val: \", max_val)\n",
    "        print(\"path: \", path)\n",
    "    \n",
    "    # Apply Z normalization \n",
    "    mean = np.mean(scaled_data)\n",
    "    std_dev = np.std(scaled_data)\n",
    "    normalized_data = (scaled_data - mean) / std_dev\n",
    "    return normalized_data\n",
    "\n",
    "def process_scan(path):\n",
    "    \"\"\"Read and resize volume\"\"\"\n",
    "    # Read scan\n",
    "    slice = read_np_file(path)\n",
    "    # Normalize\n",
    "    slice = normalize(slice, path)\n",
    "    return slice\n",
    "\n",
    "def load_npy_paths(split, group, modality):\n",
    "    \"\"\"Read each nii path regardless of the split and the group\"\"\" \n",
    "    root_path = \"/home/Data/Datasets/Parkinson/radiological/PPMI/spect-mri/filtered/\"\n",
    "    cases = sorted(os.listdir(root_path + split + \"/\" + group + \"/extension/npyFiles/\" + modality))\n",
    "    paths = []\n",
    "    for case in cases:\n",
    "        case_path = root_path + split + \"/\" + group + \"/extension/npyFiles/\" + modality + \"/\" + case + \"/\"\n",
    "        files = os.listdir(case_path)\n",
    "        \n",
    "        for file in files:\n",
    "            file_path = case_path + file\n",
    "            paths.append(file_path)\n",
    "    return paths    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate(volume):\n",
    "    \"\"\"Rotate the volume by a few degrees\"\"\"\n",
    "\n",
    "    def scipy_rotate(volume):\n",
    "        # define some rotation angles\n",
    "        angles = [-20, -10, -5, 5, 10, 20]\n",
    "        # pick angles at random\n",
    "        angle = random.choice(angles)\n",
    "        # rotate volume\n",
    "        volume = ndimage.rotate(volume, angle, reshape=False)\n",
    "        # volume[volume < 0] = 0\n",
    "        # volume[volume > 1] = 1\n",
    "        return volume\n",
    "\n",
    "    augmented_volume = tf.numpy_function(scipy_rotate, [volume], tf.float32)\n",
    "    return augmented_volume\n",
    "\n",
    "\n",
    "def train_preprocessing(volume, label):\n",
    "    \"\"\"Process training data by rotating and adding a channel.\"\"\"\n",
    "    # Rotate volume\n",
    "    #volume = rotate(volume)\n",
    "    #volume = tf.expand_dims(volume, axis=3)\n",
    "    return volume, label\n",
    "\n",
    "\n",
    "def test_preprocessing(volume, label):\n",
    "    \"\"\"Process validation data by only adding a channel.\"\"\"\n",
    "    volume = tf.expand_dims(volume, axis=3)\n",
    "    return volume, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Getting the nii path for each split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"/home/Data/Datasets/Parkinson/radiological/PPMI/spect-mri/filtered/\"\n",
    "##===== for control group\n",
    "#train\n",
    "split = \"train\"\n",
    "group = \"control\"\n",
    "modality = \"spect\"\n",
    "control_train_paths = load_npy_paths(split, group, modality)\n",
    "#test\n",
    "split = \"test\"\n",
    "control_test_paths = load_npy_paths(split, group, modality)\n",
    "\n",
    "##===== for parkinson group\n",
    "#train\n",
    "split = \"train\"\n",
    "group = \"parkinson\"\n",
    "modality = \"spect\"\n",
    "parkinson_train_paths = load_npy_paths(split, group, modality)\n",
    "#test\n",
    "split = \"test\"\n",
    "parkinson_test_paths = load_npy_paths(split, group, modality)\n",
    "\n",
    "########## labels ####################\n",
    "# assign 1 for parkinson and 0 for the control ones.\n",
    "control_train_labels = np.array([0 for _ in range(len(control_train_paths))])\n",
    "control_test_labels = np.array([0 for _ in range(len(control_test_paths))])\n",
    "parkinson_train_labels = np.array([1 for _ in range(len(parkinson_train_paths))])\n",
    "parkinson_test_labels = np.array([1 for _ in range(len(parkinson_test_paths))])\n",
    "\n",
    "## in summay:\n",
    "print(\"========== paths info ================\")\n",
    "print(\"Control train paths: \", len(control_train_paths))\n",
    "print(\"Control test paths: \", len(control_test_paths))\n",
    "print(\"Parkinson train paths: \", len(parkinson_train_paths))\n",
    "print(\"Parkinson test paths: \", len(parkinson_test_paths))\n",
    "\n",
    "print(\"========== labels info ================\")\n",
    "\n",
    "print(\"Control train labels: \", len(control_train_labels))\n",
    "print(\"Control test labels: \", len(control_test_labels))\n",
    "print(\"Parkinson train labels: \", len(parkinson_train_labels))\n",
    "print(\"Parkinson test labels: \", len(parkinson_test_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Building the train and test datagenerators**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(file_paths, labels, batch_size):\n",
    "    num_samples = len(file_paths)\n",
    "    while True:\n",
    "        # Shuffle the data for each epoch\n",
    "        indices = np.random.permutation(num_samples)\n",
    "        for start_idx in range(0, num_samples, batch_size):\n",
    "            end_idx = min(start_idx + batch_size, num_samples)\n",
    "            batch_indices = indices[start_idx:end_idx]\n",
    "            batch_files = [file_paths[i] for i in batch_indices]\n",
    "            batch_labels = labels[batch_indices]\n",
    "            \n",
    "            # Load and preprocess the batch of data\n",
    "            X_batch = []\n",
    "            for file_path in batch_files:\n",
    "                # Preprocess data as needed\n",
    "                data = process_scan(file_path)\n",
    "                X_batch.append(data)\n",
    "            X_batch = np.array(X_batch)\n",
    "            \n",
    "            # Convert NumPy arrays to TensorFlow tensors\n",
    "            X_batch_tf = tf.convert_to_tensor(X_batch)\n",
    "            y_batch_tf = tf.convert_to_tensor(batch_labels)\n",
    "            \n",
    "            yield X_batch_tf, y_batch_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "\n",
    "X_train_files = np.concatenate((control_train_paths, parkinson_train_paths), axis=0)\n",
    "y_train = np.concatenate((control_train_labels, parkinson_train_labels), axis=0)\n",
    "\n",
    "X_test_files = np.concatenate((control_test_paths, parkinson_test_paths), axis=0)\n",
    "y_test = np.concatenate((control_test_labels, parkinson_test_labels), axis=0)\n",
    "\n",
    "# Create generators for training and validation data\n",
    "train_generator = data_generator(X_train_files, y_train, batch_size)\n",
    "val_generator = data_generator(X_test_files, y_test, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Defining the 2D CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (218, 182, 3)\n",
    "base_model = tf.keras.applications.VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "        \n",
    "#making the transfer learning\n",
    "for layer in base_model.layers[:10]:\n",
    "    layer.trainable = False\n",
    "for layer in base_model.layers[10:]:\n",
    "    layer.trainable = True \n",
    "    \n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = tf.keras.layers.Flatten(name='flatten')(x)\n",
    "x = tf.keras.layers.Dense(1024, activation='relu', kernel_regularizer=regularizers.l2(0.01), name='fc1')(x)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "x = tf.keras.layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.01), name='fc2')(x)\n",
    "preds = tf.keras.layers.Dense(2, activation='softmax', name='predictions')(x)\n",
    "custom_model = Model(inputs=base_model.input, outputs=preds)\n",
    "\n",
    "print(custom_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model.\n",
    "initial_learning_rate = 0.0001\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate, decay_steps=100000, \n",
    "                                                             decay_rate=0.96, staircase=True)\n",
    "\n",
    "custom_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(), \n",
    "                     optimizer=keras.optimizers.Adam(learning_rate=0.001), metrics=[\"acc\"], run_eagerly=True)\n",
    "\n",
    "# Define callbacks.\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "    \"3d_image_classification.keras\", save_best_only=True\n",
    ")\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(monitor=\"val_acc\", patience=5)\n",
    "\n",
    "# Train the model, doing validation at the end of each epoch\n",
    "num_epochs = 100\n",
    "# Calculate the number of batches per epoch\n",
    "num_train_samples = len(X_train_files)\n",
    "num_valid_samples = len(X_test_files)\n",
    "\n",
    "num_train_steps = num_train_samples // batch_size\n",
    "num_valid_steps = num_valid_samples // batch_size\n",
    "#save model\n",
    "save_path = \"/home/Data/franklin/Doctorado/parkinson/projects/parcellation_translation/models/embc_extension/classifier/mri_spect/nii_files/\"\n",
    "\n",
    "# Redirect stdout to suppress output\n",
    "class SuppressOutput:\n",
    "    def __enter__(self):\n",
    "        self._stdout = sys.stdout\n",
    "        sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = self._stdout\n",
    "ref_val_acc = 0\n",
    "ref_val_loss = 1\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    # Training loop\n",
    "    train_loss = 0.0\n",
    "    train_accuracy = 0.0\n",
    "    for _ in range(num_train_steps):\n",
    "        X_train_batch, y_train_batch = next(train_generator)\n",
    "        with SuppressOutput():\n",
    "            train_metrics = custom_model.train_on_batch(X_train_batch, y_train_batch)\n",
    "        train_loss += train_metrics[0]\n",
    "        train_accuracy += train_metrics[1]\n",
    "    \n",
    "    train_loss /= num_train_steps\n",
    "    train_accuracy /= num_train_steps\n",
    "    print(f\"Training Loss: {train_loss}, Training Accuracy: {train_accuracy}\")\n",
    "    \n",
    "    # Validation loop\n",
    "    valid_loss = 0.0\n",
    "    valid_accuracy = 0.0\n",
    "    for _ in range(num_valid_steps):\n",
    "        X_valid_batch, y_valid_batch = next(val_generator)\n",
    "        valid_metrics = custom_model.evaluate(X_valid_batch, y_valid_batch, verbose=0)\n",
    "        valid_loss += valid_metrics[0]\n",
    "        valid_accuracy += valid_metrics[1]\n",
    "    \n",
    "    valid_loss /= num_valid_steps\n",
    "    valid_accuracy /= num_valid_steps\n",
    "    print(f\"Validation Loss: {valid_loss}, Validation Accuracy: {valid_accuracy}\")\n",
    "    if valid_loss < ref_val_loss:\n",
    "        ref_val_loss = valid_accuracy\n",
    "        #saving the model\n",
    "        model_name = \"vgg16LossReference.h5\"\n",
    "        custom_model.save(save_path+model_name)\n",
    "    if valid_accuracy > ref_val_acc:\n",
    "        ref_val_acc = valid_accuracy\n",
    "        #saving the model\n",
    "        model_name = \"vgg16AccReference.h5\"\n",
    "        custom_model.save(save_path+model_name)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
