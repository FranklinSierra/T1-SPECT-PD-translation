{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='white'>**Libraries**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\";\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix, multilabel_confusion_matrix, ConfusionMatrixDisplay \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Input,Dense,GlobalAveragePooling2D,Flatten,concatenate,BatchNormalization, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras import regularizers\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow import keras\n",
    "import imageio\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras import regularizers\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "from tqdm import tqdm\n",
    "from numpy import asarray\n",
    "import random\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn import preprocessing\n",
    "from tensorflow import keras\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>**Helper functions**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>**Data**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading csv files and dataframes by fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataframes(task):\n",
    "    \"\"\"\n",
    "    Loads data from three CSV files (training, validation, and test) into pandas DataFrames.\n",
    "\n",
    "    Returns:\n",
    "    train_df (pandas.DataFrame): DataFrame containing the training data with two columns, 'path' and 'label'.\n",
    "    val_df (pandas.DataFrame): DataFrame containing the validation data with two columns, 'path' and 'label'.\n",
    "    test_df (pandas.DataFrame): DataFrame containing the test data with two columns, 'path' and 'label'.\n",
    "    \"\"\"\n",
    "    # Set the path for the CSV files\n",
    "    print(\"working on task: \", task)\n",
    "    \n",
    "    gen_path = '../../../../../../Datasets/Parkinson/radiological/PPMI/spect-mri/filtered/' \n",
    "    synthetic_data = \"../imgs_results/full_rois/preprocessed/mri_to_spect/\"\n",
    "    \n",
    "    if task == \"control_to_pd\":\n",
    "        csv_train = os.path.join(gen_path + 'control_pd_MRI_fullRois_TRAIN.csv')\n",
    "        csv_test = os.path.join(gen_path + 'control_pd_MRI_fullRois_TEST.csv')\n",
    "    if task == \"mri_to_spect\":\n",
    "        csv_train = os.path.join(gen_path + 'embc_extension/extension_control_pd_SPECT_fullRois_TRAIN.csv')\n",
    "        csv_test = os.path.join(gen_path + 'embc_extension/extension_control_pd_SPECT_fullRois_TEST.csv')\n",
    "    else:\n",
    "        csv_train = os.path.join(synthetic_data + 'full_train_mriSpectFullRois.csv')\n",
    "        csv_test = os.path.join(synthetic_data + 'full_test_mriSpectFullRois.csv')\n",
    "        \n",
    "        \n",
    "\n",
    "    # Load the training data from the CSV file and assign column names to the DataFrame\n",
    "    train_df = pd.read_csv(csv_train, header=None)\n",
    "    train_df.columns = ['path', 'label']\n",
    "    \n",
    "    #========= here for valid set from train set =========\n",
    "    \n",
    "    # X = train_df['path']\n",
    "    # y = train_df['label']\n",
    "\n",
    "    # # Split the training data into training and validation sets (90-10 split ratio)\n",
    "    # X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=14)\n",
    "\n",
    "    # # Concatenate the training data and their labels to create the training DataFrame\n",
    "    # train_df = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "    # # Concatenate the validation data and their labels to create the validation DataFrame\n",
    "    # val_df = pd.concat([X_val, y_val], axis=1)\n",
    "    \n",
    "    #========= until here =========\n",
    "\n",
    "    # Load the test data from the CSV file and assign column names to the DataFrame\n",
    "    test_df = pd.read_csv(csv_test, header=None)\n",
    "    test_df.columns = ['path', 'label']\n",
    "\n",
    "    # Return the DataFrames containing the loaded data\n",
    "    return train_df, test_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>**Networks**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(arquitectura, height, width, num_clases):\n",
    "    \n",
    "    #general custom setup\n",
    "    first_dense_layer_neurons  = 1024 \n",
    "    second_dense_layer_neurons = 512 \n",
    "    use_global_average_pooling = True \n",
    "    use_batch_norm             = True \n",
    "    use_drop_out               = True  \n",
    "    \n",
    "    input_shape = (height, width, 3) #for adjust the rgb requirements\n",
    "    \n",
    "    print(\"cargando red: \", arquitectura)\n",
    "    if arquitectura == 'MobileNet':        \n",
    "        base_model = tf.keras.applications.MobileNet(weights='imagenet', include_top=False, \n",
    "                                                     input_shape=input_shape)\n",
    "        #making the transfer learning\n",
    "        for layer in base_model.layers[:50]:\n",
    "            layer.trainable = False\n",
    "        for layer in base_model.layers[50:]:\n",
    "            layer.trainable = True \n",
    "            \n",
    "    elif arquitectura == 'MobileNetv2':\n",
    "        base_model = tf.keras.applications.MobileNetV2(weights='imagenet', include_top=False, \n",
    "                                                input_shape=input_shape)\n",
    "        \n",
    "        #making the transfer learning\n",
    "        for layer in base_model.layers[:70]:\n",
    "            layer.trainable = False\n",
    "        for layer in base_model.layers[70:]:\n",
    "            layer.trainable = True \n",
    "            \n",
    "    elif arquitectura == 'NasNetMobile':\n",
    "        base_model = tf.keras.applications.NASNetMobile(weights='imagenet', include_top=False, \n",
    "                                                input_shape=input_shape)\n",
    "        \n",
    "        #making the transfer learning\n",
    "        for layer in base_model.layers[:500]:\n",
    "            layer.trainable = False\n",
    "        for layer in base_model.layers[500:]:\n",
    "            layer.trainable = True \n",
    "        \n",
    "    elif arquitectura == 'Vgg16':\n",
    "        base_model = tf.keras.applications.VGG16(weights='imagenet', include_top=False, \n",
    "                                                input_shape=input_shape)\n",
    "        \n",
    "        #making the transfer learning\n",
    "        for layer in base_model.layers[:10]:\n",
    "            layer.trainable = False\n",
    "        for layer in base_model.layers[10:]:\n",
    "            layer.trainable = True \n",
    "            \n",
    "    elif arquitectura == 'InceptionV3':\n",
    "        base_model = tf.keras.applications.InceptionV3(weights='imagenet',include_top=False, \n",
    "                                                input_shape=input_shape)\n",
    "        \n",
    "        #making the transfer learning\n",
    "        for layer in base_model.layers[:275]:\n",
    "            layer.trainable = False\n",
    "        for layer in base_model.layers[275:]:\n",
    "            layer.trainable = True         \n",
    "        \n",
    "    else:\n",
    "        base_model = tf.keras.applications.ResNet50(weights='imagenet',include_top=False, \n",
    "                                                input_shape=input_shape)\n",
    "        \n",
    "        #making the transfer learning\n",
    "        for layer in base_model.layers[:100]:\n",
    "            layer.trainable = False\n",
    "        for layer in base_model.layers[100:]:\n",
    "            layer.trainable = True \n",
    " \n",
    "        \n",
    "    x = base_model.output\n",
    "    \n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = tf.keras.layers.Flatten(name='flatten')(x)\n",
    "    x = tf.keras.layers.Dense(1024, activation='relu', kernel_regularizer=regularizers.l2(0.01), name='fc1')(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    x = tf.keras.layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.01), name='fc2')(x)\n",
    "    preds = tf.keras.layers.Dense(num_clases, activation='softmax', name='predictions')(x)\n",
    "        \n",
    "    # if use_global_average_pooling == True:\n",
    "    #     x=GlobalAveragePooling2D()(x)\n",
    "    # else:\n",
    "    #     x=Flatten()(x)\n",
    "\n",
    "    # if use_batch_norm:\n",
    "    #     x = BatchNormalization()(x)\n",
    "    # if use_drop_out:\n",
    "    #     x = Dropout(rate=0.5)(x)\n",
    "    # x = Dense(first_dense_layer_neurons,activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "\n",
    "    # if use_batch_norm:\n",
    "    #     x = BatchNormalization()(x)\n",
    "    # if use_drop_out:\n",
    "    #     x = Dropout(rate=0.5)(x)\n",
    "    # x = Dense(second_dense_layer_neurons,activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "\n",
    "    # if use_batch_norm:\n",
    "    #     x = BatchNormalization()(x)\n",
    "    # if use_drop_out:\n",
    "    #     x = Dropout(rate=0.5)(x)\n",
    "    # preds = Dense(num_clases, activation='softmax')(x) # final layer with softmax activation\n",
    "\n",
    "    custom_model = Model(inputs=base_model.input, outputs=preds)\n",
    "                    \n",
    "    return custom_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator(df_train, df_val, HEIGHT, WIDTH):\n",
    "    \"\"\"\n",
    "    Creates image generators for training and validation data from the given DataFrames.\n",
    "\n",
    "    Parameters:\n",
    "    df_train (pandas.DataFrame): DataFrame containing the training data with two columns, 'path' and 'label'.\n",
    "    df_val (pandas.DataFrame): DataFrame containing the validation data with two columns, 'path' and 'label'.\n",
    "    HEIGHT (int): The target height for the input images.\n",
    "    WIDTH (int): The target width for the input images.\n",
    "    tipo (str): The class mode for the generator. Can be 'binary' for binary classification or 'categorical' for multi-class.\n",
    "    batch_size (int): The batch size for the data generator.\n",
    "\n",
    "    Returns:\n",
    "    train_generator (tensorflow.python.keras.preprocessing.image.DataFrameIterator): A data generator for training data.\n",
    "    valid_generator (tensorflow.python.keras.preprocessing.image.DataFrameIterator): A data generator for validation data.\n",
    "\n",
    "    Notes:\n",
    "    - The function creates two image data generators, one for training data and one for validation data.\n",
    "    - The training data generator reads data from the DataFrame 'df_train', and the validation data generator reads\n",
    "      data from the DataFrame 'df_val'.\n",
    "    - The 'tipo' parameter determines the class mode for the generator ('binary' for binary classification or\n",
    "      'categorical' for multi-class classification).\n",
    "    - The 'target_size' parameter is set to (HEIGHT, WIDTH) to resize the images to the specified dimensions.\n",
    "    - The 'seed' parameter is set to 42 for reproducibility of random transformations applied to the images.\n",
    "    - The 'shuffle' parameter is set to True to shuffle the data during each epoch.\n",
    "    \"\"\"\n",
    "    # Generator for training data\n",
    "    shear_range = 0.1 #@param {type:\"slider\", min:0.0, max:1.0, step:0.05}\n",
    "    zoom_range = 0.1 #@param {type:\"slider\", min:0.0, max:1.0, step:0.05}\n",
    "    width_shift_range = 0.1 #@param {type:\"slider\", min:0.0, max:1.0, step:0.05}\n",
    "    height_shift_range = 0.1 #@param {type:\"slider\", min:0.0, max:1.0, step:0.05}\n",
    "    rotation_range = 10 #@param {type:\"slider\", min:0, max:90, step:5}\n",
    "    horizontal_flip = True #@param {type:\"boolean\"}\n",
    "    vertical_flip = False #@param {type:\"boolean\"}\n",
    "    \n",
    "    def custom_rescale(img):\n",
    "      return (img / 127.5) - 1.0\n",
    "   \n",
    "    datagen = ImageDataGenerator(preprocessing_function=custom_rescale,\n",
    "                                shear_range=shear_range,\n",
    "                                zoom_range=zoom_range,\n",
    "                                width_shift_range=width_shift_range,\n",
    "                                height_shift_range=height_shift_range,\n",
    "                                rotation_range=rotation_range,\n",
    "                                horizontal_flip=horizontal_flip,\n",
    "                                vertical_flip=vertical_flip)\n",
    "\n",
    "    train_generator = datagen.flow_from_dataframe(directory=None, \n",
    "                                                  dataframe=df_train,\n",
    "                                                  x_col='path', \n",
    "                                                  y_col='label', \n",
    "                                                  target_size=(HEIGHT, WIDTH),\n",
    "                                                  class_mode=\"categorical\", \n",
    "                                                  batch_size=16,\n",
    "                                                  seed=42,\n",
    "                                                  shuffle=True)\n",
    "\n",
    "    # Generator for validation data\n",
    "    val_datagen = ImageDataGenerator(preprocessing_function=custom_rescale)\n",
    "\n",
    "    valid_generator = val_datagen.flow_from_dataframe(directory=None,\n",
    "                                                      dataframe=df_val,\n",
    "                                                      x_col='path',\n",
    "                                                      y_col='label',\n",
    "                                                      batch_size=16,\n",
    "                                                      seed=42,\n",
    "                                                      shuffle=True,\n",
    "                                                      class_mode=\"categorical\",\n",
    "                                                      target_size=(HEIGHT, WIDTH))\n",
    "\n",
    "    return train_generator, valid_generator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_custom_layers(df_train, df_val, HEIGHT, WIDTH, arquitectura, clases, fine_tunning=False):\n",
    "    \n",
    "    print(\"making the generators\")\n",
    "    train_generator, valid_generator = make_generator(df_train, df_val, HEIGHT, WIDTH)\n",
    "    \n",
    "    def step_decay(epoch):\n",
    "        initial_lrate = 0.1\n",
    "        drop = 0.5\n",
    "        epochs_drop = 5.0\n",
    "        lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "        return lrate\n",
    "    \n",
    "\n",
    "    if fine_tunning == False:\n",
    "        print(\"making the custom model\")\n",
    "        custom_model = make_model(arquitectura, HEIGHT, WIDTH, clases)\n",
    "        #callbacks\n",
    "        save_path = \"../models/embc_extension/classifier/mri_spect/raw/first_approach/\" + arquitectura + \".h5\"\n",
    "        lr = 0.0001    \n",
    "        \n",
    "        custom_model.compile(optimizer=Adam(learning_rate=lr), loss='categorical_crossentropy', \n",
    "                         metrics = ['accuracy']) \n",
    "        \n",
    "    else:\n",
    "        print(\"making fine tunning\")\n",
    "        name = arquitectura\n",
    "        trained_model_path = '../models/classifier/mri_spect/preprocessed/first_approach/' + name + '.h5'\n",
    "        custom_model = keras.models.load_model(trained_model_path, compile=True)\n",
    "        \n",
    "        print(\"unfreezing all the layers\")\n",
    "        for layer in custom_model.layers:\n",
    "            layer.trainable = True\n",
    "        \n",
    "        #callbacks\n",
    "        save_path = \"../models/classifier/mri_spect/preprocessed/fine-tunning/\" + arquitectura + \"v3.h5\"\n",
    "        lr = 0.0001\n",
    "        custom_model.compile(optimizer=Adam(learning_rate=lr), loss='categorical_crossentropy', metrics = ['accuracy']) \n",
    "\n",
    "\n",
    "    if fine_tunning == True:\n",
    "        #Callbacks\n",
    "        ##############################\n",
    "        print(\"callback para refinamiento\")\n",
    "    \n",
    "        callback_list = [#tf.keras.callbacks.EarlyStopping(patience=5, monitor='val_accuracy', mode='max'),  \n",
    "                         tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n",
    "                                                            patience=4, mode='min',\n",
    "                                                             min_lr = 0.00000001)\n",
    "                        ]\n",
    "        \n",
    "        custom_model.fit_generator(generator=train_generator,\n",
    "                                steps_per_epoch=train_generator.n//train_generator.batch_size,\n",
    "                                epochs=50, \n",
    "                                validation_data=valid_generator,\n",
    "                                validation_steps=valid_generator.n//valid_generator.batch_size,\n",
    "                                #class_weight=class_weight,\n",
    "                                callbacks=callback_list)\n",
    "    else:\n",
    "        callback_list = [tf.keras.callbacks.EarlyStopping(patience=5, monitor='val_accuracy', mode='max'),                                     \n",
    "                    tf.keras.callbacks.ModelCheckpoint(filepath=save_path,\n",
    "                                                      monitor = 'val_accuracy',\n",
    "                                                      verbose=1,\n",
    "                                                      save_best_only=True,\n",
    "                                                      mode = 'max',\n",
    "                                                      save_weights_only=False,\n",
    "                                                      save_freq='epoch')]\n",
    "        custom_model.fit_generator(generator=train_generator,\n",
    "                                steps_per_epoch=train_generator.n//train_generator.batch_size,\n",
    "                                epochs=50, \n",
    "                                validation_data=valid_generator,\n",
    "                                validation_steps=valid_generator.n//valid_generator.batch_size,\n",
    "                                #class_weight=class_weight,\n",
    "                                callbacks=callback_list)\n",
    "        \n",
    "        \n",
    "    \n",
    "    #     print(\"class weights\")\n",
    "    #     total = df_train.shape[0]\n",
    "    #     weights = (total/df_train.groupby('label').count().values)/3\n",
    "    #     class_weight = {0:weights[0][0], 1:weights[1][0], 2:weights[2][0]}\n",
    "\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    return custom_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = load_dataframes(task=\"mri_to_spect\")\n",
    "train_df.groupby('label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.groupby('label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorting by case number and slice number for the following situations:\n",
    "#preprocessing: from 41 to 132\n",
    "#raw cases: 109 to 180\n",
    "\n",
    "#sorting by case number and slice number\n",
    "#train\n",
    "train_df[['case_number', 'slice_number']] = train_df['path'].str.extract(r'_case_(\\d+)_slice_(\\d+).png').astype(int)\n",
    "train_df_v2 = train_df[(train_df['slice_number'] > 41) & (train_df['slice_number'] < 132)]\n",
    "train_df_v2.drop('slice_number', axis=1, inplace=True)\n",
    "train_df_v2.drop('case_number', axis=1, inplace=True)\n",
    "\n",
    "#test\n",
    "test_df[['case_number', 'slice_number']] = test_df['path'].str.extract(r'_case_(\\d+)_slice_(\\d+).png').astype(int)\n",
    "test_df_v2 = test_df[(test_df['slice_number'] > 41) & (test_df['slice_number'] < 132)]\n",
    "test_df_v2.drop('slice_number', axis=1, inplace=True)\n",
    "test_df_v2.drop('case_number', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"lengh of train: {}, lengh of test: {}\".format(len(train_df_v2), len(test_df_v2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this cell is for a general classification knowledge\n",
    "HEIGHT, WIDTH = 256, 256\n",
    "#nets = ['Vgg16', 'MobileNet', 'ResNet50']\n",
    "nets = ['MobileNet']\n",
    "num_clases=2\n",
    "fine_tunning = False\n",
    "\n",
    "for net in nets:\n",
    "    arquitectura = net \n",
    "    np.random.seed(42)\n",
    "    tf.random.set_seed(42)\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    finetune_model = train_custom_layers(train_df_v2, test_df_v2, HEIGHT, WIDTH, arquitectura, num_clases, fine_tunning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_model.save(\"Data/franklin/Doctorado/parkinson/projects/parcellation_translation/models/classifier/mri_spect/preprocessed/fine-tunning/Vgg16v3.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "### Making generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_rescale(img):\n",
    "    return (img / 127.5) - 1.0\n",
    "                                 \n",
    "def make_generator(df_test, HEIGHT, WIDTH, batch_size):\n",
    "    \n",
    "    test_datagen=ImageDataGenerator(preprocessing_function=custom_rescale)\n",
    "    \n",
    "    test_generator=test_datagen.flow_from_dataframe(directory=None,\n",
    "                                                    dataframe=df_test,\n",
    "                                                    x_col='path',\n",
    "                                                    y_col='label',\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    seed=42,\n",
    "                                                    shuffle=False,\n",
    "                                                    class_mode=\"categorical\",\n",
    "                                                    target_size=(HEIGHT,WIDTH))\n",
    "\n",
    "    return test_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the model\n",
    "name = \"MobileNet\"\n",
    "model_path = '../models/embc_extension/classifier/mri_spect/raw/first_approach/' + name + '.h5'\n",
    "model = keras.models.load_model(model_path, compile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEIGHT, WIDTH = 256, 256\n",
    "tipo = 'categorical'\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading different kind of MRI or SPECT sources\n",
    "**Loading the SPECT synthetic images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"../imgs_results/full_rois/raw/mri_to_spect/full_test_fullRois.csv\", header=None)\n",
    "test_df.columns = ['path', 'label']\n",
    "test_df.groupby('label').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading the cycleGan subset data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_control_df = pd.read_csv(\"../data/full_rois/mri/test_control.csv\", header=None)\n",
    "test_pd_df = pd.read_csv(\"../data/full_rois/mri/test_parkinson.csv\", header=None)\n",
    "\n",
    "test_control_df.columns = ['path', 'label']\n",
    "test_pd_df.columns = ['path', 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.concat([test_control_df, test_pd_df], axis=0)\n",
    "test_df.groupby('label').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Until here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gen = make_generator(test_df_v2, HEIGHT, WIDTH, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(model.layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confution Matrix and Classification Report\n",
    "test_gen.reset()\n",
    "logits = model.predict(test_gen, test_df.shape[0] // batch_size+1)\n",
    "y_pred_class = np.argmax(logits, axis=1)\n",
    "\n",
    "target_names = ['control', 'parkinson']   \n",
    "  \n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(test_gen.classes, y_pred_class))\n",
    "print('Classification Report')\n",
    "print(classification_report(test_gen.classes, y_pred_class, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "target_names = ['control', 'parkinson']   \n",
    "cm = confusion_matrix(test_gen.classes, y_pred_class, normalize='true')\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=target_names)\n",
    "disp = disp.plot(include_values=True, cmap=plt.cm.Blues, xticks_rotation='horizontal', values_format='.2f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUC = tf.keras.metrics.AUC()\n",
    "AUC.update_state(test_gen.classes, y_pred_class)\n",
    "AUC.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
