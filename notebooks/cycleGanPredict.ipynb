{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\";\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#allows to import generator and discriminator\n",
    "!pip install -q git+https://github.com/tensorflow/examples.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow_examples.models.pix2pix import pix2pix\n",
    "from os import listdir\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from numpy import vstack\n",
    "from numpy import asarray\n",
    "from numpy import savez_compressed\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#AUTOTUNE = tf.data.AUTOTUNE\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "from PIL import Image\n",
    "import glob\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.version.VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 1000\n",
    "BATCH_SIZE = 1\n",
    "IMG_WIDTH = 256\n",
    "IMG_HEIGHT = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>**Useful methods**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all images in a directory into memory\n",
    "def load_images(path, size=(256,256)):\n",
    "    data_list = list()\n",
    "    #enumerate filenames in directory, assume all are images\n",
    "    for filename in listdir(path):\n",
    "        # load and resize the image\n",
    "        pixels = load_img(path + filename, target_size=size)\n",
    "        # convert to numpy array\n",
    "        pixels = img_to_array(pixels)\n",
    "        # store\n",
    "        data_list.append(pixels)\n",
    "    return asarray(data_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data augmentation techniques**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_crop(image):\n",
    "    cropped_image = tf.image.random_crop(image, size=[IMG_HEIGHT, IMG_WIDTH, 3])\n",
    "\n",
    "    return cropped_image\n",
    "\n",
    "# scaling the images to [-1, 1]\n",
    "def normalize(image):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = (image / 127.5) - 1\n",
    "    return image\n",
    "\n",
    "def random_jitter(image):\n",
    "    # resizing to 286 x 286 x 3\n",
    "    image = tf.image.resize(image, [286, 286],\n",
    "                          method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "\n",
    "    # randomly cropping to 256 x 256 x 3\n",
    "    image = random_crop(image)\n",
    "\n",
    "    # random mirroring\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocess splits**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image_train(image):\n",
    "    image = random_jitter(image)\n",
    "    image = normalize(image)\n",
    "    return image\n",
    "\n",
    "def preprocess_image_test(image):\n",
    "    image = normalize(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import and reuse the Pix2Pix models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_CHANNELS = 3\n",
    "\n",
    "generator_g = pix2pix.unet_generator(OUTPUT_CHANNELS, norm_type='instancenorm')\n",
    "generator_f = pix2pix.unet_generator(OUTPUT_CHANNELS, norm_type='instancenorm')\n",
    "\n",
    "discriminator_x = pix2pix.discriminator(norm_type='instancenorm', target=False)\n",
    "discriminator_y = pix2pix.discriminator(norm_type='instancenorm', target=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initializing optimizers, generator and discriminators**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_g_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "generator_f_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "\n",
    "discriminator_x_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "discriminator_y_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "\n",
    "#nbi_cls_model_optimizier = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>**Loading models**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base_model = tf.keras.models.load_model('../models/classifier/binary/MobileNet.h5', compile=True)\n",
    "#print(\"model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for layer in base_model.layers:\n",
    "    #print(layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#backbone = base_model.get_layer('mobilenet_1.00_224')\n",
    "#x = base_model.get_layer('global_average_pooling2d')(backbone.output)\n",
    "#x = base_model.get_layer('dense')(x)\n",
    "#x = base_model.get_layer('dropout')(x)\n",
    "#x = base_model.get_layer('dense_1')(x)\n",
    "#\n",
    "#nbi_cls_model = tf.keras.Model(inputs=backbone.input, outputs=x)\n",
    "#print(nbi_cls_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"../models/cyclegan/preprocessed/mri_to_spect/\"\n",
    "ckpt = tf.train.Checkpoint(generator_g=generator_g,\n",
    "                           generator_f=generator_f,\n",
    "                           discriminator_x=discriminator_x,\n",
    "                           discriminator_y=discriminator_y,\n",
    "                           generator_g_optimizer=generator_g_optimizer,\n",
    "                           generator_f_optimizer=generator_f_optimizer,\n",
    "                           discriminator_x_optimizer=discriminator_x_optimizer,\n",
    "                           discriminator_y_optimizer=discriminator_y_optimizer)\n",
    "                           #nbi_cls_model=nbi_cls_model)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print ('Latest checkpoint restored!!')\n",
    "ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    print(\"Restored from {}\".format(ckpt_manager.latest_checkpoint))\n",
    "else:\n",
    "    print(\"Initializing from scratch.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_g.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(generator_g, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>**Predicting over full test subjects**</font>\n",
    "## Main\n",
    "### Original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb2gray(path, size, pixels):\n",
    "    img2 = np.zeros((pixels.shape))\n",
    "    a = load_img(path, target_size=size, color_mode= \"grayscale\")\n",
    "    img2[:,:,0] = a\n",
    "    img2[:,:,1] = a\n",
    "    img2[:,:,2] = a\n",
    "\n",
    "    return img2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ========= here is for control to parkinson ========= ###\n",
    "# split = \"test\"\n",
    "# modality = \"parkinson\"\n",
    "# gen_path = \"../data/full_rois/mri/\" + split + \"_\" + modality + \"/\"\n",
    "# save_path = \"../imgs_results/full_rois/mri/\" + split + \"_\" + modality + \"/\"\n",
    "\n",
    "# if modality == \"control\":\n",
    "#      print(\"loading generator_g\")\n",
    "#      generator = generator_g\n",
    "# else:\n",
    "#      print(\"loading generator_f\")\n",
    "#      generator = generator_f     \n",
    "\n",
    "### ========= here is for T1 to SPECT ========= ###\n",
    "split = \"train\"\n",
    "modality = \"mri\" \n",
    "\n",
    "gen_path = \"../../../../../../Datasets/Parkinson/radiological/PPMI/spect-mri/filtered/\" + split + \"_\" + modality + \"/\"\n",
    "print(gen_path)\n",
    "save_path = \"../imgs_results/full_rois/mri_to_spect/\" + split + \"_\" + modality + \"/\"\n",
    "print(save_path)\n",
    "\n",
    "if modality == \"mri\":\n",
    "     print(\"loading generator_g\")\n",
    "     generator = generator_g\n",
    "else:\n",
    "     print(\"loading generator_f\")\n",
    "     generator = generator_f     \n",
    "\n",
    "files = sorted(os.listdir(gen_path))\n",
    "size = (256,256)\n",
    "rgb = True\n",
    "\n",
    "for filename in files:\n",
    "           \n",
    "     data_list = list()\n",
    "     general_info = filename.split('_')\n",
    "     clase = general_info[2]\n",
    "     id_img = general_info[-1]\n",
    "\n",
    "     # # load and resize the image\n",
    "     # pixels = load_img(gen_path + filename, target_size=size, color_mode= \"rgb\")\n",
    "     # # convert to numpy array\n",
    "     # pixels = img_to_array(pixels)\n",
    "\n",
    "     # if rgb==False:\n",
    "     #           #convert rgb to gray\n",
    "     #           pixels = rgb2gray(gen_path + filename, size, pixels)\n",
    "     # else:\n",
    "     #      None\n",
    "\n",
    "     # data_list.append(pixels)\n",
    "     # img_array = asarray(data_list)\n",
    "\n",
    "     # split_ds = tf.data.Dataset.from_tensor_slices(img_array)\n",
    "     # split_ds = split_ds.map(preprocess_image_test, num_parallel_calls=AUTOTUNE).batch(BATCH_SIZE)\n",
    "\n",
    "     # sample = next(iter(split_ds))\n",
    "     # fake = generator(sample)\n",
    "     # fake = fake[0]* 0.5 + 0.5\n",
    "     # #para que PIL Image deje guardar (mult por 255 and change by uint8)\n",
    "     # fake = np.array(fake) * 255\n",
    "     # fake = fake.astype(np.uint8)\n",
    "     # fake_img = Image.fromarray(fake)\n",
    "\n",
    "     # #for save\n",
    "     # directory = save_path + clase \n",
    "     # if not os.path.exists(directory):\n",
    "     #     os.makedirs(directory)\n",
    "\n",
    "     # salve_path = directory + '/' + filename\n",
    "     # fake_img.save(salve_path) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CycleGan data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_path = '../../../../../../Datasets/Parkinson/radiological/PPMI/spect-mri/filtered/' \n",
    "csv_test = os.path.join(gen_path + 'control_pd_SPECT_fullRois_TEST.csv')\n",
    "mri_test_df = pd.read_csv(csv_test, sep=',', header=None)\n",
    "mri_test_df.columns = [\"path\", \"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mri_test_df.groupby('label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "mri_test_df[['case_number', 'slice_number']] = mri_test_df['path'].str.extract(r'_case_(\\d+)_slice_(\\d+).png').astype(int)\n",
    "mri_test_df_v2 = mri_test_df[(mri_test_df['slice_number'] > 41) & (mri_test_df['slice_number'] < 132)]\n",
    "mri_test_df_v2.drop('slice_number', axis=1, inplace=True)\n",
    "mri_test_df_v2.drop('case_number', axis=1, inplace=True)\n",
    "\n",
    "print(\"len mri_tset_df_v2: \", len(mri_test_df_v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"../imgs_results/full_rois/mri_to_spect/mri_filtered_slices/\"\n",
    "\n",
    "#since we want to convert MRI to SPECT then we load the generator g\n",
    "generator = generator_g\n",
    "\n",
    "size = (256,256)\n",
    "rgb = True\n",
    "\n",
    "for i in range(len(mri_test_df_v2)):\n",
    "     \n",
    "     data_list = list()\n",
    "     \n",
    "     path = mri_test_df_v2.iloc[i]['path']\n",
    "     #/control_case_3104_slice_042.png\n",
    "     general_info = path.split('/')[-1]\n",
    "     current_general_info = general_info.split('.')[0]\n",
    "     \n",
    "     clase = current_general_info.split('_')[0]\n",
    "     id_img = current_general_info.split('_')[-1]\n",
    "     \n",
    "     # # load and resize the image\n",
    "     pixels = load_img(path, target_size=size, color_mode= \"rgb\")\n",
    "     # convert to numpy array\n",
    "     pixels = img_to_array(pixels)\n",
    "\n",
    "     if rgb==False:\n",
    "          #convert rgb to gray\n",
    "          pixels = rgb2gray(gen_path + filename, size, pixels)\n",
    "     else:\n",
    "          None\n",
    "\n",
    "     data_list.append(pixels)\n",
    "     img_array = asarray(data_list)\n",
    "\n",
    "     split_ds = tf.data.Dataset.from_tensor_slices(img_array)\n",
    "     split_ds = split_ds.map(preprocess_image_test, num_parallel_calls=AUTOTUNE).batch(BATCH_SIZE)\n",
    "\n",
    "     sample = next(iter(split_ds))\n",
    "     fake = generator(sample)\n",
    "     fake = fake[0]* 0.5 + 0.5\n",
    "     #para que PIL Image deje guardar (mult por 255 and change by uint8)\n",
    "     fake = np.array(fake) * 255\n",
    "     fake = fake.astype(np.uint8)\n",
    "     fake_img = Image.fromarray(fake)\n",
    "\n",
    "     #for save\n",
    "     directory = save_path + clase \n",
    "     if not os.path.exists(directory):\n",
    "         os.makedirs(directory)\n",
    "\n",
    "     salve_path = directory + '/' + general_info\n",
    "     fake_img.save(salve_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
