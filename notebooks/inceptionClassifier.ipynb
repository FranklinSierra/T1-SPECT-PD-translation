{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\";\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Imports. {display-mode:'form'}\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input,Dense,GlobalAveragePooling2D,Flatten,concatenate,BatchNormalization, Dropout\n",
    "from tensorflow.keras.applications import InceptionV3,DenseNet121\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# Visualize the Train/Val loss\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the data generators\n",
    "### Loading the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_path = '../../../../../../Datasets/Parkinson/radiological/PPMI/spect-mri/filtered/' \n",
    "    \n",
    "csv_train = os.path.join(gen_path + 'control_pd_SPECT_fullRois_TRAIN.csv')\n",
    "csv_test = os.path.join(gen_path + 'control_pd_SPECT_fullRois_TEST.csv')\n",
    "\n",
    "# Load the training data from the CSV file and assign column names to the DataFrame\n",
    "train_df = pd.read_csv(csv_train, header=None)\n",
    "train_df.columns = ['path', 'label']\n",
    "\n",
    "# Load the test data from the CSV file and assign column names to the DataFrame\n",
    "test_df = pd.read_csv(csv_test, header=None)\n",
    "test_df.columns = ['path', 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.groupby('label').count())\n",
    "print(test_df.groupby('label').count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Set the data generators. {display-mode:'form', run: \"auto\"}\n",
    "#@markdown Data augmentation choices. Cell runs automatically if anything is changed.\n",
    "shear_range = 0.1 #@param {type:\"slider\", min:0.0, max:1.0, step:0.05}\n",
    "zoom_range = 0.1 #@param {type:\"slider\", min:0.0, max:1.0, step:0.05}\n",
    "width_shift_range = 0.1 #@param {type:\"slider\", min:0.0, max:1.0, step:0.05}\n",
    "height_shift_range = 0.1 #@param {type:\"slider\", min:0.0, max:1.0, step:0.05}\n",
    "rotation_range = 10 #@param {type:\"slider\", min:0, max:90, step:5}\n",
    "horizontal_flip = True #@param {type:\"boolean\"}\n",
    "vertical_flip = False #@param {type:\"boolean\"}\n",
    "#@markdown Data source (No need to change if the download succeeded.)\n",
    "\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                    shear_range=shear_range,\n",
    "                                    zoom_range=zoom_range,\n",
    "                                    width_shift_range=width_shift_range,\n",
    "                                    height_shift_range=height_shift_range,\n",
    "                                    rotation_range=rotation_range,\n",
    "                                    horizontal_flip=horizontal_flip,\n",
    "                                    vertical_flip=vertical_flip)\n",
    "\n",
    "train_generator = train_datagen.flow_from_dataframe(directory=None,\n",
    "                                                    dataframe=train_df,\n",
    "                                                    x_col='path', \n",
    "                                                    y_col='label',\n",
    "                                                    target_size=(256,256),\n",
    "                                                    color_mode='rgb',\n",
    "                                                    batch_size=8,\n",
    "                                                    class_mode='categorical',\n",
    "                                                    shuffle=True)\n",
    "\n",
    "# Data Generator for validation without data augmentation!\n",
    "val_datagen   = ImageDataGenerator(rescale=1./255)\n",
    "val_generator = val_datagen.flow_from_dataframe(directory=None,\n",
    "                                                 dataframe=test_df,\n",
    "                                                 x_col='path', \n",
    "                                                 y_col='label',\n",
    "                                                 target_size=(256,256),\n",
    "                                                 color_mode='rgb',\n",
    "                                                 batch_size=8,\n",
    "                                                 class_mode='categorical',\n",
    "                                                 shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the pretrained model and add dense layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Set up the pretrained model, and add dense layers. {display-mode:'form', run: \"auto\"}\n",
    "#@markdown Set up the trainable dense layers. Further options include BatchNorm (provides regularization), DropOut (also, for normalization, but should not be used together with BatchNorm), and GlobalAveragePooling as an alternative to simple flattening. Did not work well in our experiments.  Cell runs automatically if anything is changed.\n",
    "first_dense_layer_neurons  = 1024 #@param {type:\"integer\"}\n",
    "second_dense_layer_neurons = 256 #@param {type:\"integer\"}\n",
    "use_global_average_pooling = False #@param {type:\"boolean\"}\n",
    "use_batch_norm             = True #@param {type:\"boolean\"}\n",
    "use_drop_out               = False  #@param {type:\"boolean\"}\n",
    "pretrained_model           = 'Inception V3' #@param [\"Inception V3\", \"DenseNet 121\"]\n",
    "optimizer                  = 'adam' #@param ['adam', 'adagrad', 'adadelta', 'sgd'] {allow-input: true}\n",
    "\n",
    "if pretrained_model == 'Inception V3':\n",
    "    base_model=InceptionV3(weights='imagenet',include_top=False, input_shape=(256,256,3))\n",
    "else:\n",
    "    base_model=DenseNet121(weights='imagenet',include_top=False, input_shape=(256,256,3))\n",
    "\n",
    "x=base_model.output\n",
    "\n",
    "if use_global_average_pooling == True:\n",
    "    x=GlobalAveragePooling2D()(x)\n",
    "else:\n",
    "    x=Flatten()(x)\n",
    "\n",
    "if use_batch_norm:\n",
    "    x = BatchNormalization()(x)\n",
    "if use_drop_out:\n",
    "    x = Dropout(rate=0.5)(x)\n",
    "x = Dense(first_dense_layer_neurons,activation='relu')(x)\n",
    "\n",
    "if use_batch_norm:\n",
    "    x = BatchNormalization()(x)\n",
    "if use_drop_out:\n",
    "    x = Dropout(rate=0.5)(x)\n",
    "x = Dense(second_dense_layer_neurons,activation='relu')(x)\n",
    "\n",
    "if use_batch_norm:\n",
    "    x = BatchNormalization()(x)\n",
    "if use_drop_out:\n",
    "    x = Dropout(rate=0.5)(x)\n",
    "preds = Dense(2,activation='softmax')(x) # final layer with softmax activation\n",
    "\n",
    "model = Model(inputs=base_model.input,outputs=preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First pass: train addedd dense layers\n",
    "First train only the top layers (randomly initialized) freezing all convolutional InceptionV3 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "if optimizer in ['adam', 'adagrad', 'adadelta', 'sgd']: # standard settings\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics = ['accuracy']) # categorical crossentropy would also do...\n",
    "else:\n",
    "    from tensorflow.keras.optimizers import SGD\n",
    "    model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(generator=train_generator,\n",
    "                              steps_per_epoch=train_generator.n//train_generator.batch_size,\n",
    "                              epochs=30, # Originally, 500 epochs!\n",
    "                             validation_data=val_generator,\n",
    "                             validation_steps=val_generator.n//val_generator.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Plot train and validation loss/accuracy {display-mode:'form'}\n",
    "\n",
    "plt.plot(history.history['loss'], label='train loss')\n",
    "plt.plot(history.history['val_loss'], label='val loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Depending on the version of TF/Keras, the metric is either stored as 'acc' or 'accuracy'. This is not checked here.\n",
    "plt.plot(history.history['accuracy'], label='train acc')\n",
    "plt.plot(history.history['val_accuracy'], label='val acc')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tune last convolutional layers\n",
    "The added layers should have converged. It is a good practice to fine-tune the top convolutional layers. \n",
    "\n",
    "In this case we chose to fine-tune the top 2 inception blocks, i.e. we will freeze the first 249 layers and unfreeze the rest. Afterwards, the model needs to be recompiled.\n",
    "\n",
    "Note that this will not change the trained parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Set up trainable parameters {display-mode:'form'}\n",
    "for i, layer in enumerate(base_model.layers):\n",
    "    print(i, layer.name)\n",
    "\n",
    "for layer in model.layers[:249]:\n",
    "    layer.trainable = False\n",
    "for layer in model.layers[249:]:\n",
    "    layer.trainable = True\n",
    "    \n",
    "from tensorflow.keras.optimizers import SGD\n",
    "model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_finetune = model.fit_generator(generator=train_generator,\n",
    "                                       steps_per_epoch=train_generator.n//train_generator.batch_size,\n",
    "                                       epochs=20,\n",
    "                                       validation_data=val_generator,\n",
    "                                       validation_steps=val_generator.n//val_generator.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Plot train and validation loss/accuracy {display-mode:'form'}\n",
    "\n",
    "plt.plot(history.history['loss'], label='train loss')\n",
    "plt.plot(history.history['val_loss'], label='val loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Depending on the version of TF/Keras, the metric is either stored as 'acc' or 'accuracy'. This is not checked here.\n",
    "plt.plot(history.history['accuracy'], label='train acc')\n",
    "plt.plot(history.history['val_accuracy'], label='val acc')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "#Confution Matrix and Classification Report\n",
    "val_generator.reset()\n",
    "logits = model.predict(val_generator)\n",
    "y_pred_class = np.argmax(logits, axis=1)\n",
    "#predicted_class_probab=np.max(logits,axis=1)\n",
    "\n",
    "target_names = ['control', 'parkinson']   \n",
    "  \n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(val_generator.classes, y_pred_class))\n",
    "print('Classification Report')\n",
    "print(classification_report(val_generator.classes, y_pred_class, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
